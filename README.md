Below is a single text-file you can paste into “docminer_repo_layout.txt”, print, e-mail, or just follow line-by-line.  
It contains:

A. Repository / folder structure  
B. File contents (dev-container, Function code, requirements, GitHub Action, …)  
C. Short “how it works” notes

Everything is secret-free; keep keys in Azure Key Vault or local.settings.json (NOT committed).

????????????????????????????????????????????????????????????
docminer_repo_layout.txt
????????????????????????????????????????????????????????????
ROOT (GitHub repo name: docminer-pipeline)
?
?? .devcontainer/
?  ?? devcontainer.json
?
?? .github/
?  ?? workflows/
?     ?? deploy_az_func.yml
?
?? src/
?  ?? DocMinerFunctionApp/          # Azure Function project
?     ?? __init__.py
?     ?? function.json
?     ?? host.json
?     ?? requirements.txt
?     ?? .funcignore
?
?? .gitignore
?? README.md
????????????????????????????????????????????????????????????
File: .devcontainer/devcontainer.json
????????????????????????????????????????????????????????????
{
  "name": "DocMiner Codespace",
  "image": "mcr.microsoft.com/devcontainers/python:0-3.11",
  "features": {
    "ghcr.io/devcontainers/features/azure-cli:1": {},
    "ghcr.io/devcontainers/features/github-cli:1": {}
  },
  "customizations": {
    "vscode": {
      "settings": { "python.defaultInterpreterPath": "/usr/local/bin/python" },
      "extensions": [
        "ms-python.python",
        "ms-azuretools.vscode-azurefunctions"
      ]
    }
  },
  "postCreateCommand": "pip install -r src/DocMinerFunctionApp/requirements.txt"
}

????????????????????????????????????????????????????????????
File: .github/workflows/deploy_az_func.yml
????????????????????????????????????????????????????????????
name: Build & Deploy Function App

on:
  push:
    branches: [ "main" ]
    paths:
    # Only redeploy when function source or deps change
      - "src/DocMinerFunctionApp/**"
      - ".github/workflows/deploy_az_func.yml"

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    steps:
    - name: Checkout
      uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: "3.11"

    - name: Install deps
      run: |
        pip install -r src/DocMinerFunctionApp/requirements.txt

    - name: ‘Zip Function App’
      run: |
        cd src
        zip -r ../function.zip DocMinerFunctionApp

    - name: Deploy to Azure
      uses: Azure/functions-action@v1
      with:
        app-name: ${{ secrets.AZURE_FUNCTIONAPP_NAME }}
        package: function.zip
        publish-profile: ${{ secrets.AZURE_FUNCTIONAPP_PUBLISHPROFILE }}
        scm-do-build-during-deployment: true

????????????????????????????????????????????????????????????
File: src/DocMinerFunctionApp/requirements.txt
????????????????????????????????????????????????????????????
azure-functions
azure-storage-blob
azure-ai-documentintelligence
azure-search-documents
azure-cosmos
azure-identity          # (managed identity optional)
python-dotenv           # local dev only
????????????????????????????????????????????????????????????
File: src/DocMinerFunctionApp/function.json
????????????????????????????????????????????????????????????
{
  "scriptFile": "__init__.py",
  "bindings": [
    {
      "name": "myblob",
      "type": "blobTrigger",
      "direction": "in",
      "path": "uploads/{name}",
      "connection": "documentminer_STORAGE"
    }
  ]
}
????????????????????????????????????????????????????????????
File: src/DocMinerFunctionApp/host.json   (minimal)
????????????????????????????????????????????????????????????
{
  "version": "2.0",
  "logging": {
    "applicationInsights": { "samplingSettings": { "isEnabled": true } }
  }
}
????????????????????????????????????????????????????????????
File: src/DocMinerFunctionApp/__init__.py
????????????????????????????????????????????????????????????
import os, json, logging, uuid, time
import azure.functions as func
from azure.storage.blob import BlobClient
from azure.cosmos import CosmosClient
from azure.search.documents import SearchClient
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.core.credentials import AzureKeyCredential

# -----------------------------------------------------------------------------
# Load configuration from environment / local.settings.json / Key Vault
# -----------------------------------------------------------------------------
STORAGE_CS     = os.getenv("STORAGE_CS")          # connection string
COSMOS_EP      = os.getenv("COSMOS_URI")
COSMOS_KEY     = os.getenv("COSMOS_KEY")
COSMOS_DB      = "LoanParticipation"
DI_ENDPOINT    = os.getenv("DI_ENDPOINT")         # https://docminerloandocservice...
DI_KEY         = os.getenv("DI_KEY")
SEARCH_EP      = os.getenv("SEARCH_EP")
SEARCH_KEY     = os.getenv("SEARCH_KEY")
SEARCH_INDEX   = "docminer-idx"

# -----------------------------------------------------------------------------
# Azure clients
# -----------------------------------------------------------------------------
cosmos_client  = CosmosClient(COSMOS_EP, COSMOS_KEY)
queries_con    = cosmos_client.get_database_client(COSMOS_DB) \
                              .get_container_client("Queries")
results_con    = cosmos_client.get_database_client(COSMOS_DB) \
                              .get_container_client("Results")

di_client      = DocumentIntelligenceClient(
                    DI_ENDPOINT, AzureKeyCredential(DI_KEY))

search_client  = SearchClient(endpoint=SEARCH_EP,
                              index_name=SEARCH_INDEX,
                              credential=AzureKeyCredential(SEARCH_KEY))

# -----------------------------------------------------------------------------
# Function declaration (blob trigger)
# -----------------------------------------------------------------------------
app = func.FunctionApp()

@app.function_name(name="NewDocTrigger")
@app.blob_trigger(arg_name="myblob",
                  path="uploads/{name}",
                  connection="documentminer_STORAGE")
def main(myblob: func.InputStream):
    blob_name = myblob.name            # container/uploads/filename.pdf
    size      = myblob.length
    logging.info(f"Triggered on {blob_name} ({size} bytes)")

    # ----------------------------------------------------------------------
    # 1. Analyse document with Document Intelligence (prebuilt-read)
    # ----------------------------------------------------------------------
    poller  = di_client.begin_analyze_document("prebuilt-read", myblob.read())
    di_res  = poller.result()
    raw_json = di_res.to_dict()

    # ----------------------------------------------------------------------
    # 2. Fetch “prompts” from Cosmos container Queries
    #    Expected schema: {id:<uuid>, prompt:<question_string>}
    # ----------------------------------------------------------------------
    prompts = list(queries_con.read_all_items())

    # Prepare answers list
    answers = []
    # Naïve approach: answer = first page text snippet containing keyword
    # Replace with smarter logic or OpenAI retrieval when ready.
    pages_text = {p.page_number: p.content for p in di_res.pages}

    for q in prompts:
        question = q.get("prompt")
        found = ""
        for txt in pages_text.values():
            if question.lower() in txt.lower():
                found = txt
                break
        answers.append({
            "promptId": q["id"],
            "prompt"  : question,
            "answer"  : found[:4000]     # cap to 4k
        })

    # ----------------------------------------------------------------------
    # 3. Persist aggregated JSON to Cosmos Results
    # ----------------------------------------------------------------------
    results_con.upsert_item({
        "id"         : str(uuid.uuid4()),
        "file"       : os.path.basename(blob_name),
        "blobPath"   : blob_name,
        "questions"  : answers,
        "diRawPath"  : f"di-output/{os.path.basename(blob_name)}.json",
        "status"     : "processed",
        "timestamp"  : time.time()
    })

    # Optional: store DI raw JSON in a separate container for debugging
    try:
        BlobClient.from_connection_string(
            STORAGE_CS,
            container_name="di-output",
            blob_name=f"{os.path.basename(blob_name)}.json"
        ).upload_blob(json.dumps(raw_json), overwrite=True)
    except Exception as e:
        logging.warning(f"Could not persist DI output: {e}")

    logging.info(f"Completed processing {blob_name}")

????????????????????????????????????????????????????????????
File: src/DocMinerFunctionApp/.funcignore
????????????????????????????????????????????????????????????
# Exclude secrets & build artifacts
local.settings.json
*.pyc
__pycache__/
.diRaw/

????????????????????????????????????????????????????????????
File: .gitignore  (root)
????????????????????????????????????????????????????????????
# Byte-compiled, secrets, envs
*.pyc
__pycache__/
.env
local.settings.json
function.zip
.vscode/
.python-packages/
????????????????????????????????????????????????????????????
File: README.md  (excerpt)
????????????????????????????????????????????????????????????
# DocMiner Pipeline

End-to-end Azure Function that reacts to new blobs, runs Document
Intelligence, marries results with predefined “prompts” stored in Cosmos DB
and writes answers back to Cosmos.

## Quick start in GitHub Codespaces
1. Click “Code ? Codespaces ? Create codespace”.
2. Wait for dev-container to build; dependencies auto-install.
3. Run locally  
   ```bash
   cd src/DocMinerFunctionApp
   func start
   ```
   (Make sure `local.settings.json` contains your secrets.)

## CI/CD
GitHub Action `.github/workflows/deploy_az_func.yml`
deploys only when files inside `src/DocMinerFunctionApp` change.

---

Replace the naive Q/A section in `__init__.py` with hybrid search + Azure
OpenAI for higher accuracy.

????????????????????????????????????????????????????????????
HOW IT ALL FITS TOGETHER
????????????????????????????????????????????????????????????
• Developer opens Codespace ? VS Code remote container has Azure CLI, Python.  
• `func start` runs Function locally for testing.  
• Pushing to `main` auto-zips `src/DocMinerFunctionApp` and uses the
  `Azure/functions-action@v1` to deploy; redeployment happens only if
  `src/DocMinerFunctionApp/**` changes (path filter).  
• At runtime the Function:
    1. Fires on new file `uploads/{name}`.  
    2. Calls Document Intelligence prebuilt-read model on file stream.  
    3. Reads questions from Cosmos container `Queries`.  
    4. Produces rudimentary answers (stub logic).  
    5. Writes combined JSON headed by the original file name to
       Cosmos container `Results` and stores raw DI output in blob
       container `di-output`.

You can now copy the block above into a `.txt` file or follow it directly
to scaffold your project. Enjoy building!
# docminer-azure
